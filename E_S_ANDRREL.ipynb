{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKnlAhI6q+OBgRcgQTcv36",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANDRREL/python-projects/blob/main/E_S_ANDRREL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPnSya7AnKDj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "Xf5HmXUCUKVW",
        "outputId": "3d330eb1-cc26-42b1-9b1e-e478c58f373f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Index(['partner', 'course', 'skills', 'rating', 'reviewcount', 'level',\n",
            "       'certificatetype', 'duration', 'crediteligibility'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'date'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1573969084.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Corrected typo: pd.to_Datetime should be pd.to_datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'storeid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'productid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'date'"
          ]
        }
      ],
      "source": [
        "# SALES FORECASTING REPORT FOR RETAIL STORES\n",
        "\n",
        "#importing the datas\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import holidays\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "\n",
        "# Make sure this file path is correct and the file exists in your Google Drive\n",
        "df= pd.read_csv('/content/drive/MyDrive/Coursera.csv')\n",
        "df.columns=df.columns.str.lower()\n",
        "\n",
        "# Print column names to identify the date column\n",
        "print(df.columns)\n",
        "\n",
        "# Corrected typo: pd.to_Datetime should be pd.to_datetime\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.sort_values(['storeid','productid','date']).reset_index(drop=True)\n",
        "\n",
        "df.head()\n",
        "\n",
        "#data checks\n",
        "print(df.isna().sum())\n",
        "print(df[['storeid','productid']].nunique())\n",
        "print(df['date'].min(), df['date'].max())\n",
        "df['sales'] = df['sales'].fillna(0)\n",
        "\n",
        "\n",
        "country_holidays = holidays.country_holidays('IN')\n",
        "\n",
        "dcal = pd.DataFrame({'Date': pd.date_range(df['date'].min(), df['date'].max())})\n",
        "dcal['dow']   = dcal['Date'].dt.dayofweek\n",
        "dcal['dom']   = dcal['Date'].dt.day\n",
        "dcal['week']  = dcal['Date'].dt.isocalendar().week.astype(int)\n",
        "dcal['month'] = dcal['Date'].dt.month\n",
        "dcal['year']  = dcal['Date'].dt.year\n",
        "dcal['is_weekend'] = (dcal['dow']>=5).astype(int)\n",
        "dcal['is_holiday'] = dcal['Date'].isin(country_holidays).astype(int)\n",
        "\n",
        "df = df.merge(dcal, left_on='date', right_on='Date', how='left').drop(columns='Date')\n",
        "df.head()\n",
        "\n",
        "def add_lags(group, lags=(1,7,28), rolls=(7,28)):\n",
        "    for l in lags:\n",
        "        group[f'lag_{l}'] = group['sales'].shift(l)\n",
        "    for r in rolls:\n",
        "        group[f'rollmean_{r}'] = group['sales'].shift(1).rolling(r).mean()\n",
        "        group[f'rollstd_{r}']  = group['sales'].shift(1).rolling(r).std()\n",
        "    return group\n",
        "\n",
        "df = df.groupby(['storeid','productid'], as_index=False).apply(add_lags).reset_index(drop=True)\n",
        "df = df.dropna(subset=[c for c in df.columns if c.startswith('lag_') or c.startswith('roll')])\n",
        "df.head()\n",
        "\n",
        "cutoff = df['date'].max() - pd.Timedelta(days=90)\n",
        "train = df[df['date'] <= cutoff].copy()\n",
        "valid = df[df['date'] >  cutoff].copy()\n",
        "\n",
        "features = [\n",
        "    'dow','dom','week','month','year','is_weekend','is_holiday',\n",
        "    'lag_1','lag_7','lag_28','rollmean_7','rollmean_28','rollstd_7','rollstd_28'\n",
        "]\n",
        "\n",
        "# Encode storeid/productid as categories for XGBoost\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_storeid, le_productid = LabelEncoder(), LabelEncoder()\n",
        "df['storeid_le'] = le_storeid.fit_transform(df['storeid'])\n",
        "df['productid_le']  = le_productid.fit_transform(df['productid'])\n",
        "\n",
        "features += ['storeid_le','productid_le']\n",
        "\n",
        "train = df[df['date'] <= cutoff].copy()\n",
        "valid = df[df['date'] >  cutoff].copy()\n",
        "\n",
        "X_train, y_train = train[features], train['sales']\n",
        "X_valid, y_valid = valid[features], valid['sales']\n",
        "\n",
        "\n",
        "s, i = df['storeid'].iloc[0], df['productid'].iloc[0]\n",
        "ts = df[(df['storeid']==s) & (df['productid']==i)][['date','sales']].rename(columns={'date':'ds','sales':'y'})\n",
        "\n",
        "from prophet import Prophet\n",
        "m = Prophet()\n",
        "m.fit(ts)\n",
        "future = m.make_future_dataframe(periods=30)  # 30-day horizon\n",
        "fcst = m.predict(future)[['ds','yhat','yhat_lower','yhat_upper']]\n",
        "fcst.tail()\n",
        "\n",
        "model = XGBRegressor(\n",
        "    n_estimators=800,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.9,\n",
        "    random_state=42,\n",
        "    tree_method=\"hist\"\n",
        ")\n",
        "model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
        "\n",
        "pred_valid = model.predict(X_valid)\n",
        "mae  = mean_absolute_error(y_valid, pred_valid)\n",
        "rmse = mean_squared_error(y_valid, pred_valid)\n",
        "mae, rmse\n",
        "\n",
        "# future forecast of stock\n",
        "H = 30\n",
        "future_Dates = pd.date_range(df['date'].max() + pd.Timedelta(days=1), periods=H)\n",
        "\n",
        "\n",
        "pairs = df[['storeid','productid','storeid_le','productid_le']].drop_duplicates()\n",
        "future = pairs.assign(key=1).merge(pd.DataFrame({'Date':future_Dates,'key':1}), on='key').drop('key', axis=1)\n",
        "\n",
        "\n",
        "future = future.merge(dcal, on='Date', how='left')\n",
        "\n",
        "\n",
        "hist_window = df[['storeid','productid','date','sales']].copy()\n",
        "\n",
        "def build_future_features(g):\n",
        "    g = g.sort_values('date')\n",
        "    maxd = g['date'].max()\n",
        "\n",
        "    f = future[(future['storeid']==g['storeid'].iloc[0]) & (future['productid']==g['productid'].iloc[0])].copy()\n",
        "    f['sales'] = np.nan\n",
        "    both = pd.concat([g[['date','sales']], f[['Date','sales']]], ignore_index=True)\n",
        "    both = both.sort_values('Date')\n",
        "\n",
        "\n",
        "    for l in (1,7,28):\n",
        "        both[f'lag_{l}'] = both['sales'].shift(l)\n",
        "    for r in (7,28):\n",
        "        both[f'rollmean_{r}'] = both['sales'].shift(1).rolling(r).mean()\n",
        "        both[f'rollstd_{r}']  = both['sales'].shift(1).rolling(r).std()\n",
        "\n",
        "\n",
        "    both = both[both['Date'].isin(f['Date'])]\n",
        "    out = f.merge(both.drop(columns='sales'), on='Date', how='left', suffixes=('',''))\n",
        "    return out\n",
        "\n",
        "future_feats = (\n",
        "    hist_window.groupby(['storeid','productid'], group_keys=False)\n",
        "    .apply(build_future_features)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "\n",
        "future_feats = future_feats.merge(pairs[['storeid','productid','storeid_le','productid_le']].drop_duplicates(), on=['storeid','productid'], how='left')\n",
        "future_feats = future_feats.merge(dcal, left_on='date', right_on='Date', how='left', suffixes=('','_cal')).drop(columns='Date_cal')\n",
        "\n",
        "X_future = future_feats[features].copy()\n",
        "\n",
        "X_future = X_future.fillna(0)\n",
        "\n",
        "future_preds = model.predict(X_future)\n",
        "future_out = future_feats[['date','storeid','productid']].copy()\n",
        "future_out['forecast'] = np.maximum(0, future_preds)\n",
        "\n",
        "future_out.head()\n",
        "\n",
        "# exporting to powerBI\n",
        "valid_export = valid[['partner','course','skills','rating','reviewcount','level','reviewcount','level']].copy()\n",
        "valid_export['forecast'] = pred_valid\n",
        "valid_export.to_csv('/content/drive/MyDrive/Coursera.csv', index=False)\n",
        "future_out.to_csv('/content/drive/MyDrive/Coursera.csv', index=False)\n",
        "df[['partner','course','skills','rating','reviewcount','level']].to_csv('/content/drive/MyDrive/Coursera.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn surprise\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "df = pd.read_csv(\"Coursera.csv\")   # Load your dataset\n",
        "df.head()\n",
        "# Combine useful features\n",
        "df[\"features\"] = df[\"title\"] + \" \" + df[\"tags\"].fillna(\"\")\n",
        "\n",
        "# Convert text into vectors\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "feature_matrix = vectorizer.fit_transform(df[\"features\"])\n",
        "\n",
        "# Compute similarity\n",
        "cosine_sim = cosine_similarity(feature_matrix, feature_matrix)\n",
        "\n",
        "# Function to recommend similar courses\n",
        "def recommend_content(course_title, top_n=5):\n",
        "    idx = df[df[\"title\"] == course_title].index[0]\n",
        "    scores = list(enumerate(cosine_sim[idx]))\n",
        "    scores = sorted(scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
        "    courses = [df.iloc[i[0]][\"title\"] for i in scores]\n",
        "    return courses\n",
        "\n",
        "print(\"üìö Content-based recommendations for 'Python for Beginners':\")\n",
        "print(recommend_content(\"Python for Beginners\"))\n",
        "# Prepare data for Surprise library\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(df[[\"partner\", \"course\", \"reviewcount\"]], reader)\n",
        "\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Train SVD model\n",
        "model = SVD()\n",
        "model.fit(trainset)\n",
        "\n",
        "# Predict ratings\n",
        "predictions = model.test(testset)\n",
        "\n",
        "# Function to recommend courses for a user\n",
        "def recommend_collaborative(partner, top_n=5):\n",
        "    all_courses = df[\"course\"].unique()\n",
        "    user_courses = df[df[\"partner\"] == partner][\"course\"].unique()\n",
        "    remaining_courses = [c for c in all_courses if c not in user_courses]\n",
        "\n",
        "    preds = [(cid, model.predict(partner, cid).est) for cid in remaining_courses]\n",
        "    preds = sorted(preds, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    return df[df[\"course\"].isin([cid for cid, _ in preds])][\"title\"].tolist()\n",
        "\n",
        "print(\"ü§ù Collaborative recommendations for user U1:\")\n",
        "print(recommend_collaborative(\"U1\"))\n",
        "def hybrid_recommendation(partner, course_title, top_n=5):\n",
        "    content_recs = recommend_content(course_title, top_n=10)\n",
        "    collab_recs = recommend_collaborative(partner, top_n=10)\n",
        "    final_recs = list(set(content_recs) | set(collab_recs))[:top_n]\n",
        "    return final_recs\n",
        "\n",
        "print(\"üåü Hybrid recommendations for U1 who liked 'Python for Beginners':\")\n",
        "print(hybrid_recommendation(\"U1\", \"Python for Beginners\"))\n"
      ],
      "metadata": {
        "id": "m0emXWFInay1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}